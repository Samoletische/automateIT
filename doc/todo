Концепция
* Сервер - центральный узел. Хранит настройки (сборка, отчётность и т. д. ). Раздаёт задания (сборщикам, мониторингу,
  подготовке отчётности, анализатору, преобразователю данных для прогрузки, прогрузчику, чистильщику). Рассчитывает ЗП клиентов.
  Отслеживает каждый шаг каждого клиентского узла.
* Настройки Сервера постоянно автотестируются на пригодность. Если что-то изменилось, дают знать о необходимости правки.
* Web-ресурсы не должны понимать, что к ним заходит робот. Т. е. роботы должны вести себя как обычные люди. Исключение -
  только объёмы.
* Клиентские узлы держат в курсе Сервер по каждому из своих шагов. Только общее состояние Клиента, а не Сборщиков.
* Клиент-Сборщик (Web) управляет своими Сборщикам (Spider), отслеживая каждый шаг. Сборщики умеют собирать данные с любых
  web-ресурсов.

Текущая работа:
- пункты, которые нужно сделать
+ выполненные пункты
х пункты, которые не нужно делать
-------- Система --------
- при запуске сокетов Сборщиков не делать паузу в 3 секунды перед попыткой подключения, а сделать несколько попыток подключений
через каждые 0.5 секунд.
- разобраться почему Сборщик собирает данные не в той кодировке. Это останавливает работу на этапе processing.
Проблема в том, что один сборщик делает сразу два задания (по-порядку): dromLexus (windows-1251) и runsim21 (utf-8). Поэтому
в первом задании русский текст берётся в utf-8.
- разобраться почему статус на spyd два раза ready вместо второго раза - error/collecting.
Результат: нужно, чтобы сокет успевал отработать запрос прежде нового запроса.???
- отследить и исправить ошибку в Spider::process - иногда к вызову этой функции в $this->collector значение NULL
+ отследить и исправить отсутствие первого подключения к сокетам Сборщиков при создании Клиента.
Не было паузы между отправкой команды создать сокет и подключением к сокету. Сокет не успевал создаться. Поставил пока 3 сек.
- доработать сбор со статичных страниц.
- в Spider::process продумать в какие моменты функция может возвращать false для дальнейшего прекращения работы
с собранными данными (не вызывать storage)
- создать html-страницу Клиента. Страница подключается к сокету Клиента (starter).
  На странице отображаются:
  - состояние Сервера - есть ли к нему подключение.
  - состояние Клиента - текущий статус.
  - состояние локальной базы данных (адрес, есть ли подключение).
  - панель Заданий Клиента (файлы web.json, monitor.json). При необходимости возможен поиск заданий на сервере. Так же должна
  быть возможность создать новое задание здесь же.
  - все известные Клиенту Сборщики. Состояние их запуска (запущены ли сокеты). Статусы текущей работы (задание, статус).
  - отчётность.
  Страница получает статусы Сборщиков от сокета Клиента, НЕ запрашивает их сама. Сокет Клиента отправляет статусы только
  тогда, когда есть подключение от html-страницы.
- создать html-страницу Сервера. Страница подключается к сокету Сервера.
  На странице отображаются:
  - состояние базы данных
  - задания, их статус, время последнего тестирования, необходимость в правке (если тестирование не прошло).
  - клиенты с их статусами
  - отчётность
- сделать сокет Сервера. Будет обмениваться командами с сокетами Клиентов и передавать команды html-странице Сервера.
- перед началом выполнения задания проверять наличие необходимых таблиц в БД. При необходимости создавать их.
- заменить функцию Spider::getExistingElement на wait->until от Selenium'а
- для отлова исключения о ненахождении элемента(ов) по селектору использовать InvalidSelectorException
- скачивание данных ссылок (href) и источников (src) с учётом их расположения (http, ftp, data:base64, и т. д.)
  + base64
  - http
  - ftp
- рефакторинг класса Spider
- обязательно разработать диаграмму поведения (алгоритм) парсинга, чтобы была схема для дальнейшей модернизации
- научить авторизоваться, вставить эту настройку в web.json
- система смены ip-адреса прокси для обмана сайтов-целей
- на подумать: добавить в поля для записи в базу константы - значения, которые не зависят от значений элементов, и присутствуют всегда с указанным в настройках значением в каждой записи
- наладить нормальное поведение методов collect и monitor в зависимости от параметров collectAllData и insertOnly
+ вставить проверку готовности Selenium-сервера в Spider::ReadyToUse.
+ для определения готовности сервера Selenium можно отлавливать исключение WebDriverCurlException
+ рассмотреть возможность избавиться от локальных $params и $result в классе Spiders
х убрать у класса Spider свойство $this->currPage. Заполнять его прямо в $this->params
+ если количество родительских элементов меньше чем firstItemIndex, то можно переходить к следующей странице
+ ввести новый параметр paginationHaveSameAddress, чтобы определять как брать следующую страницу: через адрес в пагинаторе или через
параметры firstItemIndex и maxItemsCollect
+ в функциях, вызываемых через HTTP-интерфейс вместо echo (другого вывода в стандартный вывод) использовать логирование в базу
+ Web не должен зависеть от params. В starter'е и System::createWeb заменить создание Web без параметров, а параметры подсовывать через метод, и затем скармливать их Сборщикам.
+ разобраться с потерей данных в массиве при переборе (foreach) другого списка
+ если не нужны действия на странице, брать сразу её код, вставить эту настройку в web.json
+ научить скролить окно, вставить эту настройку в web.json
+ добавить везде проверку на наличие необходимых ключей в json-форматах. При отсутствии какого-либо, вызывать своё исключение. А выше отлавливать его и корректно выходить.
+ разработать систему логов
+ система поведения пользователя (паузы, наведение мыши на произвольные объекты, и т. д.)
+ система сохранения данных
+ разработать формат входных данных для парсинга (см. файл /docs/info)
+ реструктурировать код парсинга, чтобы принимал данные из json-файла
+ обработка исключений. почему-то не срабатывают...

-------- Мониторинг спроса --------
- Принципы мониторинга:
  - собрать информацию о входящих на страницу (ip-адрес; URL-адрес источник)
  - собрать информацию о том, кто из них заинтересовался (перешёл на страницу с вводом данных)
  - собрать информацию о том, кто из них решился (ввёл данные и заказал услугу)
- Страницы сайта-мониторинга:
  - завлекающая. Красиво, заинтересованно о всех услугах системы. Кнопки "хочу".
  - информационная. Должна заинтересовать ещё больше. Ввод адреса электронной почты. Кнопка "готов".
  - извеняющая. Информация о том, что сайт находится на реконструкции, сервис временно остановлен, будет извещение о возобновлении работы.
- нужен временный домен. Имя такое, чтобы можно было использовать и для др. мониторингов спроса.
- сайт сделать на конструкторе сайтов. Там же разместить или скопировать шаблон и сделать свой.

Цели:
- до 20.04.2019 сделать заготовку для сбора данных с сайтов. Чтобы с ней можно было выходить на рынок услуг по парсингу.
- до 24.04.2019 сделать сайт-мониторинг спроса услуг:
  - сбора информации (парсинг)
  - автоматизации действий
  - накрутка статистики
- до 27.04.2019 начать мониторинг - разместить сайт-мониторинг на хостинге, настроить AdWords. На площадках фрилансеров разместить свои услуги.
- до 01.05.2019 мониторить рынок. Принять решение заниматься этим дальше или нет.
//----------------------------------------
